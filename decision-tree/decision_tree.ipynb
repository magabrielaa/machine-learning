{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5adf4bac95bdabd79618a6df5d4c449c",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Mar√≠a Gabriela Ayala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89c3d2ca10ae3db20e452980b30c8513",
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "999a59eb16aacdd0d546c483f159c402",
     "grade": false,
     "grade_id": "1intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Problem 1 - Decision tree\n",
    "***\n",
    "\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Chicago residency. \n",
    "The dataset looks like the following.\n",
    "\n",
    "| Age   | Salary         | Chicago Residency      | College degree| \n",
    "|:------:|:------------:| :-----------:|---:|\n",
    "| 27 | 41,000 | Yes | Yes |\n",
    "| 61 | 52,000 | No | No |\n",
    "| 23 | 24,000 | Yes | No |\n",
    "| 29 | 77,000 | Yes | Yes |\n",
    "| 32 | 48,000 | No | Yes |\n",
    "| 57 | 120,000 | Yes | Yes |\n",
    "| 22 | 38,000 | Yes | Yes |\n",
    "| 41 | 45,000 | Yes | No |\n",
    "| 53 | 26,000 | No | No |\n",
    "| 48 | 65,000 | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57ff8a7b88e7fbb838a663145288e261",
     "grade": false,
     "grade_id": "1q",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part A [5 points]**: Convert the above table to data. Two variables should be created:\n",
    "        \n",
    "1. $x$ is a $10*3$ matrix that contains the data from columns 0, 1, and 2. Chicago residency is represented by 1 (yes) and 0 (no).\n",
    "2. $y$ contains the labels (college degree), 1 (yes) and 0 (no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99d49c4279d566943cb0e9e1eed98c4c",
     "grade": true,
     "grade_id": "1a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[27, 41000, 1], [61, 52000, 0], [23, 24000, 1], [29, 77000, 1], [32, 48000, 0], [57, 120000, 1], [22, 38000, 1], [41, 45000, 1], [53, 26000, 0], [48, 65000, 1]])\n",
    "y = np.array([[1], [0], [0], [1], [1], [1], [1], [0], [0], [1]])\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a05258269121b3b60d48adaf038dc676",
     "grade": false,
     "grade_id": "2q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part B:** Criteria for choosing a feature to split.\n",
    "\n",
    "We start with no splitting. Assuming that our algorithm is deterministic, what is the smallest number of mistakes we can make if we do not use any of the features and what is the algorithm? (**Write your answer in the Markdown cell below.**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "483fd69f569c413806fa39b054f7baea",
     "grade": true,
     "grade_id": "2a1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "If we don't use any of the features, then with no splitting the algorithm would predict \"Yes\"/1 for college degree because there are 6 entries labeled 1 and 4 labeled 0. This means that the smallest number of mistakes we can make without using any of the features is 4, which corresponds to the 4 entries labeled as \"No\"/0 that would incorrectly be classified as \"Yes\"/1.\n",
    "The algorithm would count the instances of 0 and 1 labels in the y matrix and determine the max value as the default prediction label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2636e94124c66892be090f27550d92c6",
     "grade": false,
     "grade_id": "2q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We start by considering the variable *Chicago residency*. The first criteria is based on the number of mistakes. We need to build a contingency table between Chicago residency and college degree.\n",
    "\n",
    "How many mistakes will we make if we split based on Chicago residency? (**Answer below by finishing the code.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faa4e220d79ed9e1f956512242efdbd2",
     "grade": false,
     "grade_id": "2a2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_error_in_leaf(y, ids):\n",
    "    \"\"\"\n",
    "    Returns the errors in a leaf node of a decision tree.\n",
    "    This function can be used to answer the previous question automatically.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param ids: the subset of indexes in the leaf node\n",
    "    \"\"\"\n",
    "    ids = list(range((len(y))))\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "\n",
    "    for i in ids:\n",
    "        if y[i][0] == 0:\n",
    "            count_0 +=1\n",
    "        elif y[i][0] == 1:\n",
    "            count_1 += 1\n",
    "    return min(count_0, count_1)\n",
    "\n",
    "\n",
    "def error_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Returns the number of errors if we split the root into the left child and the right child.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param left_child: the subset of indexes in the left child\n",
    "    :@param right_child: the subset of indexes in the right child\n",
    "    \"\"\"\n",
    "\n",
    "    left_0 = 0\n",
    "    left_1 = 0\n",
    "\n",
    "    for lidx in left_child:\n",
    "        if y[root[lidx]][0] == 0:\n",
    "            left_0 +=1\n",
    "        elif y[root[lidx]][0] == 1:\n",
    "            left_1 += 1\n",
    "    left_error = min(left_0, left_1)\n",
    "\n",
    "    right_0 = 0\n",
    "    right_1 = 0\n",
    "\n",
    "    for ridx in right_child:\n",
    "        if y[root[ridx]][0] == 0:\n",
    "            right_0 +=1\n",
    "        elif y[root[ridx]][0] == 1:\n",
    "            right_1 += 1\n",
    "    right_error = min(right_0, right_1)\n",
    "\n",
    "    return left_error + right_error\n",
    "\n",
    "\n",
    "\n",
    "def value_split_binary_feature(x, y, fid, root, criteria_func):\n",
    "    \n",
    "    root = root.tolist()\n",
    "    left_child = [i for i in root if x[i, fid] == 0]\n",
    "    right_child = [i for i in root if x[i, fid] == 1]\n",
    "    return criteria_func(y, root, left_child, right_child)\n",
    "\n",
    "# Chicago residency should correspond to the third column in your data x\n",
    "fid = 2\n",
    "root = np.array(list(range(len(y)))) # root includes all data points\n",
    "mistakes = value_split_binary_feature(x, y, fid, root, error_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ded6d069a782d1483c34462a10b4a60f",
     "grade": false,
     "grade_id": "2a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part C** Alternatively, we can use entropy and information gain to split the data. In this part, you will manually determine the first split in a decision tree. Please do not use code in your calculations for part C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e85886de3472cd141af6569380005b1",
     "grade": false,
     "grade_id": "cell-e5a060788c99dc75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To get familiar with MathJax, please write the equations necessary to compute entropy and information gain if we split data $D$ into $D_1$ and $D_2$. **Write your answer in the Markdown cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b17e7d719a579c48373397b74c8f91f1",
     "grade": true,
     "grade_id": "cell-f6f2bbd3029c7b93",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "[1] Split the data using entropy:\n",
    "$$H(X) = -p\\space log_2p - (1 - p)log_2(1 - p)$$\n",
    "where:\n",
    "$p = \\frac{D_1}{D}$ assuming $D_1$ is the positive class, ie. 1 in a binary 1, 0 or 1, -1 relationship.\n",
    "$\\newline$\n",
    "Here:\n",
    "$\\newline$\n",
    "$0\\leq p \\leq 1$\n",
    "\n",
    "[2] Split the data using information gain, based on feature i:\n",
    "\n",
    "$\\newline$\n",
    "Where:\n",
    "$\\newline$\n",
    "\n",
    "$$X_{parent} = X_{D}$$\n",
    "$$X_{i,left} = X_{D_1, left}$$\n",
    "$$X_{i,right} = X_{D_2, right}$$\n",
    "\n",
    "The information gain formula is:\n",
    "$$IG(X_{D,i}) = H(X_{D}) - \\frac{|X_{i,D_1}|}{|X_{D}|} H(X_{D_1}) - \\frac{|X_{i,D_2}|}{|X_{D}|} H(X_{D_2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26e8b99c69ae0b16d74c1eec8c585c36",
     "grade": false,
     "grade_id": "cell-ff4b2e7471a3a466",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** What is the entropy for College Degree? Please show your work. Were you expecting a result around this number? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f00d4adb7e6c48089bb624820b851270",
     "grade": true,
     "grade_id": "cell-9954ce044843f7c0",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$p = \\frac{6}{10} = \\frac{3}{5}$$\n",
    "$$H(X) = -p\\space log_2 p - (1 - p)log_2(1 - p)$$\n",
    "$$H(X) = -\\frac{3}{5}log_2\\frac{3}{5} - (1 - \\frac{3}{5})log_2(1 - \\frac{3}{5})$$\n",
    "$$H(X) = -\\frac{3}{5} \\space log_2\\frac{3}{5} - (\\frac{2}{5})log_2(\\frac{2}{5})$$\n",
    "$$H(X) = 0.9709505944546686 $$\n",
    "\n",
    "I was expecting a number close to 1, since the labels for college degree are almost balanced, with 6 entries classified as \"Yes\"/1 and 4 entries classified as \"No\"/0. Out of a total of 10, 6-4 is very close to a perfect balance 5-5, therefore the result matches my expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d793934e3c2719314e0d5ca3f261580",
     "grade": false,
     "grade_id": "cell-6c10c241a34d72a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** What is the information gain for College Degree if you split the observations on the Chicago Residency attribute? Please show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7897d6345e22a78e2f84097f9c25e34c",
     "grade": true,
     "grade_id": "cell-b465dc36f5c4b733",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$X_{parent} = X_{D} = \\{1, 0, 0, 1, 1, 1, 1, 0, 0, 1\\}$$\n",
    "$$X_{i,left} = X_{Chicago Residency, left} = \\{0, 1, 0\\} $$\n",
    "$$X_{i,right} = X_{Chicago Residency, right} = \\{1, 0, 1, 1, 1, 0, 1\\}$$\n",
    "\n",
    "Entropy for $X_{parent}$:\n",
    "$$H(X_{parent}) = 0.9709505944546686 $$\n",
    "\n",
    "Entropy for $X_{Chicago Residency, left}$:\n",
    "$$p = \\frac{1}{3}$$\n",
    "$$H(X_{Chicago Residency, left}) = -p\\space log_2 p - (1 - p)log_2(1 - p)$$\n",
    "$$H(X_{Chicago Residency, left})  = -\\frac{1}{3}\\space log_2\\frac{1}{3} - (1 - \\frac{1}{3})log_2(1 - \\frac{1}{3})$$\n",
    "$$H(X_{Chicago Residency, left})  = -\\frac{1}{3} \\space log_2\\frac{1}{3} - (\\frac{2}{3})log_2(\\frac{2}{3})$$\n",
    "$$H(X_{Chicago Residency, left}) = 0.9182958340544896 $$\n",
    "\n",
    "Entropy for $X_{Chicago Residency, right}$:\n",
    "$$p = \\frac{5}{7}$$\n",
    "$$H(X_{Chicago Residency, right}) = -p\\space log_2 p - (1 - p)log_2(1 - p)$$\n",
    "$$H(X_{Chicago Residency, right})  = -\\frac{5}{7}\\space log_2\\frac{5}{7} - (1 - \\frac{5}{7})log_2(1 - \\frac{5}{7})$$\n",
    "$$H(X_{Chicago Residency, right})  = -\\frac{5}{7} \\space log_2\\frac{5}{7} - (\\frac{2}{7})log_2(\\frac{2}{7})$$\n",
    "$$H(X_{Chicago Residency, right}) = 0.863120568566631 $$\n",
    "\n",
    "\n",
    "Information Gain calculation:\n",
    "$$IG(X,Chicago Residency) = H(X_{parent}) - \\frac{|X_{Chicago Residency,left}|}{|X_{parent}|} H(X_{Chicago Residency, left}) - \\frac{|X_{Chicago Residency, right}|}{|X_{parent}|} H(X_{Chicago Residency, right})$$\n",
    "$$IG(X,Chicago Residency) = 0.9709505944546686 - (3/10)* 0.9182958340544896  - (7/10)* 0.863120568566631$$\n",
    "$$IG(X,Chicago Residency) = 0.0912774462416801$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32ca26854e72561a6f71ee150a5680e4",
     "grade": false,
     "grade_id": "cell-2421ce359be01217",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One way to deal with continuous (or ordinal) features like Age and Salary is to define binary features based on thresholding. For example, you might convert ages to 0 if Age is less than or equal to 50 and 1 otherwise. What is the information gain for College Degree if we split the observations based on the Salary attribute with a threshold of \\$50,000? Please show your work.\n",
    "\n",
    "(Later in this problem, we will write code to determine whether a number other than \\$50,000 might be the optimal threshold for Salary. For now, just assume that a threshold of \\\\$50,000 is optimal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a1b31913c3305f377ec4d130273967a",
     "grade": true,
     "grade_id": "cell-e8a8ac44a50254a6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Updating the Salary feature with a threshold of $50,000, we get the following table:\n",
    "| Age   | Salary         | Chicago Residency      | College degree| \n",
    "|:------:|:------------:| :-----------:|---:|\n",
    "| 27 | 0 | Yes | 1 |\n",
    "| 61 | 1 | No | 0 |\n",
    "| 23 | 0 | Yes | 0 |\n",
    "| 29 | 1 | Yes | 1 |\n",
    "| 32 | 0 | No | 1 |\n",
    "| 57 | 1 | Yes | 1 |\n",
    "| 22 | 0| Yes | 1 |\n",
    "| 41 | 0| Yes | 0 |\n",
    "| 53 | 0 | No | 0 |\n",
    "| 48 | 1| Yes | 1 |\n",
    "\n",
    "To calculate the information gain for College Degree splitting on the Salary attribute:\n",
    "$$X_{parent} = X_{D} = \\{1, 0, 0, 1, 1, 1, 1, 0, 0, 1\\}$$\n",
    "$$X_{i,left} = X_{Salary, left} = \\{1, 0, 1, 1, 0, 0\\} $$\n",
    "$$X_{i,right} = X_{Salary, right} = \\{0, 1, 1, 1\\}$$\n",
    "\n",
    "Entropy for $X_{parent}$:\n",
    "$$H(X_{parent}) = 0.9709505944546686 $$\n",
    "\n",
    "Entropy for $X_{Salary, left}$:\n",
    "$$p = \\frac{3}{6} = \\frac{1}{2} $$\n",
    "$$H(X_{Salary, left}) = -p\\space log_2 p - (1 - p)log_2(1 - p)$$\n",
    "$$H(X_{Salary, left})  = -\\frac{1}{2}\\space log_2\\frac{1}{2} - (1 - \\frac{1}{2})log_2(1 - \\frac{1}{2})$$\n",
    "$$H(X_{Salary, left})  = -\\frac{1}{2} \\space log_2\\frac{1}{2} - (\\frac{1}{2})log_2(\\frac{1}{2})$$\n",
    "$$H(X_{Salary, left}) = 1.0 $$\n",
    "\n",
    "Entropy for $X_{Salary, right}$:\n",
    "$$p = \\frac{3}{4}$$\n",
    "$$H(X_{Salary, right}) = -p\\space log_2 p - (1 - p)log_2(1 - p)$$\n",
    "$$H(X_{Salary, right})  = -\\frac{3}{4}\\space log_2\\frac{3}{4} - (1 - \\frac{3}{4})log_2(1 - \\frac{3}{4})$$\n",
    "$$H(X_{Salary, right})  = -\\frac{3}{4} \\space log_2\\frac{3}{4} - (\\frac{1}{4})log_2(\\frac{1}{4})$$\n",
    "$$H(X_{Salary, right}) =  0.8112781244591328 $$\n",
    "\n",
    "Information Gain calculation:\n",
    "$$IG(X,Salary) = H(X_{parent}) - \\frac{|X_{Salary,left}|}{|X_{parent}|} H(X_{Salary, left}) - \\frac{|X_{Salary, right}|}{|X_{parent}|} H(X_{Salary, right})$$\n",
    "$$IG(X,Salary) = 0.9709505944546686 - (6/10)* 1.0 - (4/10)* 0.8112781244591328$$\n",
    "$$IG(X,Salary) = 0.04643934467101546$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f5cfb38f586649920a233e6fae487dc",
     "grade": false,
     "grade_id": "cell-a98cc6a9b99b8b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on the information gain calculations, should the first split in our decision tree be on Chicago Residency or on Salary with a \\$50,000 threshold? Is this the answer you expected based on the data counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10707aa247b7a01d06f3200326ba16c0",
     "grade": true,
     "grade_id": "cell-06c07fbc2fc63a2a",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Based on the information gain calculations, the first split in the decision tree should be Chicago Residency because it yields a higher information gain than splitting on Salary: 0.091 > 0.046.\n",
    "This is the answer I expected based on the data counts, since the combined entropy for Chicago Residency is lower than that for Salary. The left and right child data for Chicago Residency is split between 1/3 and 5/7 whereas it is split between 3/6 and 3/4 for Salary, such that it is more imbalanced for Chicago Residency and therefore there is more to be learned by splitting first by this attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7266539b6061dc3ba06ce9fc40acc432",
     "grade": false,
     "grade_id": "cell-b90188ff50c2eaf9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part D** Now we will write a function for computing information gain. Use log2 for entropy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a112f7e33530182c36e6fc39e8b74421",
     "grade": false,
     "grade_id": "cell-bb989c485308fcc4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0912774462416801"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(y, ids):\n",
    "    \"\"\"\n",
    "    Returns the entropy in the labels for the data points in ids.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param ids: the indexes of data points\n",
    "    \"\"\"\n",
    "    if len(ids) == 0: # deal with corner case when there is no data point.\n",
    "        return 0\n",
    "    \n",
    "    count_1 = 0 \n",
    "    for i in ids:\n",
    "        if y[i][0] == 1:\n",
    "            count_1 += 1\n",
    "    \n",
    "    p = count_1/len(y)\n",
    "\n",
    "    return -(p)*math.log2(p) - (1-p)*math.log2(1-p)\n",
    "\n",
    "    \n",
    "def information_gain_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Returns the information gain by splitting root into left child and right child.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param left_child: the subset of indexes in the left child\n",
    "    :@param right_child: the subset of indexes in the right child\n",
    "    \"\"\"\n",
    "    ids = list(range((len(y))))\n",
    "    parent_entropy = entropy(y, ids)\n",
    "\n",
    "    if len(left_child) == 0 or len(right_child) == 0:\n",
    "        return 0\n",
    "\n",
    "    left_1 = 0\n",
    "\n",
    "    for lidx in left_child:\n",
    "        if y[root[lidx]][0] == 1:\n",
    "            left_1 += 1\n",
    "\n",
    "    p_left = left_1/len(left_child)\n",
    "    if p_left == 0 or p_left == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        left_entropy = -(p_left)*math.log2(p_left) - (1-p_left)*math.log2(1-p_left)\n",
    "\n",
    "    right_1 = 0\n",
    "\n",
    "    for ridx in right_child:\n",
    "        if y[root[ridx]][0] == 1:\n",
    "            right_1 += 1\n",
    "    p_right = right_1/len(right_child)\n",
    "    if p_right == 0 or p_right == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        right_entropy = -(p_right)*math.log2(p_right) - (1-p_right)*math.log2(1-p_right)\n",
    "\n",
    "\n",
    "    return  parent_entropy - len(left_child)/len(y)* left_entropy  - len(right_child)/len(y)* right_entropy\n",
    "    \n",
    "fid = 2\n",
    "root = np.array(list(range(len(y)))) # root includes all data points\n",
    "info_gain = value_split_binary_feature(x, y, fid, root, information_gain_criteria)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e69b435c5de4b7b8f56db1d1d6c5ab1",
     "grade": true,
     "grade_id": "cell-1059c1f151862da1",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbcb9570e6845117b277658fc4bc30c9",
     "grade": false,
     "grade_id": "cell-7a340274d5eccbf2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part E**: Deal with continuous features.\n",
    "    \n",
    "Complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64363b21761e99525c42238eb1d677a3",
     "grade": false,
     "grade_id": "cell-94945cb8ee7f6b14",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_split_continuous_feature(x, y, fid, root, criteria_func=information_gain_criteria):\n",
    "    \"\"\"\n",
    "    Return the best value and its corresponding threshold by splitting based on a continuous feature.\n",
    "\n",
    "    :@param x: all feature values\n",
    "    :@param y: all labels\n",
    "    :@param fid: feature id to split the tree based on\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param criteria_func: the splitting criteria function\n",
    "    \"\"\"\n",
    "    best_value, best_thres = 0, 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x1= np.copy(x)\n",
    "        threshold = x1[root[i]][fid]\n",
    "        for i in root:\n",
    "            if x1[i][fid] > threshold:\n",
    "                x1[i][fid] = 1\n",
    "            else:\n",
    "                x1[i][fid] = 0\n",
    "        left_child = [i for i in root if x1[i][fid] == 0]\n",
    "        right_child = [i for i in root if x1[i][fid] == 1]\n",
    "        info_gain = information_gain_criteria(y, root, left_child, right_child)\n",
    "        if info_gain > best_value:\n",
    "            best_value = info_gain\n",
    "            best_thres = threshold\n",
    "         \n",
    "    return best_value, best_thres\n",
    "\n",
    "root = np.array(range(len(y))) # root includes all data points\n",
    "fid = 0\n",
    "age_value, age_thres = value_split_continuous_feature(x, y, fid, root, information_gain_criteria)\n",
    "fid = 1\n",
    "salary_value, salary_thres = value_split_continuous_feature(x, y, fid, root, information_gain_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efec5b53ab6827b916bc478118b3d9a8",
     "grade": false,
     "grade_id": "cell-c7507cb413cfd74c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Based on the current information gain by splitting different features, if we build a decision stump (decision tree with depth 1) greedily, which feature should we choose? Why? **Write down your answer in the Markdown cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4202e9f4e5a2074deb181ae4002b6d3",
     "grade": true,
     "grade_id": "cell-db1dde1ecdde83ee",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "From the continuous features analysis, the best threshold for age is at 32 with an information gain of 0.124, compared to a best threshold of $45.000 for salary the same information gain of 0.124. In both continuous variable cases, best threshold yields the exact same information gain. Alternatively, the previously calculated Chicago Residency yields an information gain of 0.091.\n",
    "From a decision stump approach, either age or salary at their optimal respective thresholdcould be the first split since they yield the highest information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e746c872c2d4569bea86e57d75a58e7",
     "grade": false,
     "grade_id": "q1-extra-credit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Extra credit**: You now have all the ingredients to build a decision tree recursively. You can build a decision tree of depth two and report its classification error on the training data and the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af1dff830e3ca2ac0dde968735b78ae2",
     "grade": false,
     "grade_id": "a1-extra-credit",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LeafNode:\n",
    "    \"\"\"\n",
    "    Class for leaf nodes in the decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, label, count, total):\n",
    "        \"\"\"\n",
    "        :@param label: label of the leaf node\n",
    "        :@param count: number of data points with class 'label' falling in this leaf\n",
    "        :@param count: number of datapoints of any label falling in this leaf\n",
    "        \"\"\"\n",
    "        self.label = label\n",
    "        self.count = count\n",
    "        self.total = total\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return predictions for features x\n",
    "\n",
    "        :@param x: feature values\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def display(self, feat_names, out_str, depth=0):\n",
    "        \"\"\"\n",
    "        Display contents of a leaf node\n",
    "        \"\"\"\n",
    "        prefix = '\\t'*depth\n",
    "        error = 1.0 - self.count / float(self.total)\n",
    "        out_str += f'{prefix}leaf: label={self.label}, error={error} ({self.count}/{self.total} correct)\\n'\n",
    "        return out_str\n",
    "    \n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Class for internal (non-leaf) nodes in the decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_id, feat_val):\n",
    "        \"\"\"\n",
    "        :@param feat_id: index of the feature that this node splits on\n",
    "        :@param feat_val: threshold for the feature that this node splits on\n",
    "        \"\"\"\n",
    "        self.feat_id = feat_id\n",
    "        self.feat_val = feat_val\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def split(self, x, root):\n",
    "        \"\"\"\n",
    "        Given the datapoints falling into current node, return two arrays of indices in x corresponding to the\n",
    "        left and right subtree\n",
    "        \n",
    "        :@param x: all feature values\n",
    "        :@param root: indexes of all the data points in the current node\n",
    "        \"\"\"\n",
    "        root = np.array(root)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return an array of predictions for given 'x' for the current node\n",
    "        \n",
    "        :@param x: datapoints\n",
    "        \"\"\"\n",
    "        assert self.left is not None and self.right is not None, 'predict called before fit'\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def display(self, feat_names, out_str, depth=0):\n",
    "        \"\"\"\n",
    "        Display contents of a non-leaf node\n",
    "        \"\"\"\n",
    "        prefix = '\\t'*depth\n",
    "        out_str += f'{prefix}{feat_names[self.feat_id]}\\n'\n",
    "        out_str += f'{prefix}x <= {self.feat_val}\\n'\n",
    "        out_str = self.left.display(feat_names, out_str, depth=depth+1)\n",
    "        out_str += f'{prefix}x > {self.feat_val}\\n'\n",
    "        out_str = self.right.display(feat_names, out_str, depth=depth+1)\n",
    "        return out_str\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Class for the decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=1, criteria_func=information_gain_criteria, binary_feat_ids=[]):\n",
    "        \"\"\"\n",
    "        :@param max_depth: Maximum depth that a decision tree can take\n",
    "        :@param criteria_func: criteria function to split features\n",
    "        :@param binary_feat_id: list of indexes of binary features\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criteria_func = criteria_func\n",
    "        self.binary_feat_ids = binary_feat_ids\n",
    "        self.root = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit a tree to the given dataset using a helper function\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.root = self.fit_helper(np.array(list(range(self.x.shape[0]))))\n",
    "    \n",
    "    def fit_helper(self, root, depth=1):\n",
    "        \"\"\"\n",
    "        Recursive helper function for fitting a decision tree\n",
    "        Returns a node (can be either LeafNode or TreeNode)\n",
    "        \n",
    "        :@param root: array of indices of datapoints which fall into the current node\n",
    "        :@param depth: current depth of the tree being built \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Strategy:\n",
    "        1. If current partition is pure i.e. labels corresponding to all indices in root are the same\n",
    "           OR the maximum depth has been reached, stop building the tree and return a LeafNode\n",
    "        2. If not, find out the best feature to split on along with the threshold, create a TreeNode and \n",
    "           recursively call fit_helper on the two splits (You can assume the threshold for a binary feature \n",
    "           to be 0.5). Finally, return the current node \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return predictions for a given dataset  \n",
    "        \"\"\"\n",
    "        assert self.root is not None, 'fit not yet called'\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def display(self, feat_names):\n",
    "        assert self.root is not None, 'fit not yet called'\n",
    "        out_str = \"\"\n",
    "        out_str = self.root.display(feat_names, out_str)\n",
    "        return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22fc2d2915109be0a17a61e7b488d418",
     "grade": true,
     "grade_id": "cell-24ff429d1f9a70fc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f46d05b143e83eb978885711367e74ae",
     "grade": false,
     "grade_id": "cell-44230b1deea50132",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Crime Prediction \n",
    "\n",
    "Download [reported crime data for 2021](https://data.cityofchicago.org/Public-Safety/Crimes-2021/dwme-t96c) from the Chicago Open Data Portal (we already prepared it in the data folder, but would you to know where the source was).\n",
    "\n",
    "**Part A**: Load the data.  We will only use three columns of the dataset: Primary Type, Latitude, and Longitude.\n",
    "- Be sure to drop any observations that are missing latitude and/or longitude.\n",
    "- To reduce run time, only keep incidents reported as one of the the four most common crime types ('THEFT', 'BATTERY', 'CRIMINAL DAMAGE', 'ASSAULT'). \n",
    "- Use the train_test_split function from Scikit-Learn to split the data into training and validation sets.  Set random_state=123 and test_size=0.2.\n",
    "- Finally, explore the training and validation sets and answer the following questions: \n",
    "  - How many total observations are in the training set? \n",
    "  - How many total observations are in the validation set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7fde237a9ed9fec31582dc1c58abd8e",
     "grade": false,
     "grade_id": "cell-50674b47f654cd5f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Write code for answering the questions in Part A and then put your answer in the Markdown cell below.\n",
    "df = pd.read_csv(\"crimes.csv\")[['Primary Type', 'Latitude', 'Longitude']]\n",
    "df = df.dropna(subset=['Latitude', 'Latitude'])\n",
    "df = df.loc[(df['Primary Type'] == 'THEFT') | (df['Primary Type'] == 'BATTERY') |(df['Primary Type'] == 'CRIMINAL DAMAGE') | (df['Primary Type'] == 'ASSAULT')]\n",
    "train, validate = train_test_split(df, test_size=0.2, random_state=123)\n",
    "X_train = train[['Latitude', 'Longitude']]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = train[['Primary Type']]\n",
    "y_train = y_train.to_numpy()\n",
    "# Make sure to set each of the variables below to the correct value. Do not rename the variables.\n",
    "N_training_examples = 99749\n",
    "N_validation_examples = 24938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b50c3b1db910c49ecb8650de370f042",
     "grade": true,
     "grade_id": "cell-738b21d7f854fdb0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "There are 99749 observations in the training set and 24938 observations in the validation/test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
